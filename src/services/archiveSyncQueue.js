import localforage from 'localforage';
import { useArchiveStore } from '@/stores/archiveStore';
import { useAuthStore } from '@/stores/auth';
import { useHarvestStore } from '@/stores/harvest';
import { useNotifications } from '@/composables/useNotifications';
import { useSyncStore } from '@/stores/syncStore'; // Import new store
import logger from '@/utils/logger.js';
import { supabase } from '@/supabase';
import api from '@/services/api';

export const QUEUE_KEY = 'archive_sync_queue'; // Exported for store usage

// Ù…ØªØºÙŠØ± Ù„Ù…Ù†Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ²Ø§Ù…Ù†Ø© (Race Condition Protection)
let isProcessing = false;
let processingPromise = null;

/**
 * Ø¥Ø¶Ø§ÙØ© Ø¹Ù…Ù„ÙŠØ© (Ø­ÙØ¸ Ø£Ùˆ Ø­Ø°Ù) Ø¥Ù„Ù‰ Ø·Ø§Ø¨ÙˆØ± Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
 */
async function addToSyncQueue(item) {
  try {
    let queue = (await localforage.getItem(QUEUE_KEY)) || [];

    const type = item.type || 'daily_archive';
    const source = item.payload || item;
    const date = source.archive_date || source.date;

    if (!date) {
      logger.warn('âš ï¸ SyncQueue: Item missing date, skipping.', { item });
      return;
    }

    // ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù†ÙØ³ Ø§Ù„ØªØ§Ø±ÙŠØ®
    const existsIndex = queue.findIndex(q => {
      const qSource = q.payload || q;
      const qDate = qSource.archive_date || qSource.date;
      return q.type === type && qDate === date;
    });

    if (existsIndex === -1) {
      queue.push(item);
      await localforage.setItem(QUEUE_KEY, queue);
      logger.info(`ğŸ“Œ Added to sync queue: [${type}] for ${date}`);
    } else {
      // ØªØ­Ø¯ÙŠØ« Ù„Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØªØºÙŠØ± Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§
      if (type === 'daily_archive' || type === 'sync_overdue_stores') {
        queue[existsIndex] = item;
        await localforage.setItem(QUEUE_KEY, queue);
        logger.info(`ğŸ”„ Updated sync queue: [${type}] for ${date}`);
      }
      // delete_archive Ù„Ø§ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø¯ÙŠØ« (Ø¹Ù…Ù„ÙŠØ© Ø­Ø°Ù Ø¨Ø³ÙŠØ·Ø©)
    }

    // Update Sync Status UI
    const syncStore = useSyncStore();
    syncStore.checkQueue();

    // Trigger processing immediately if online
    if (navigator.onLine) {
      // Ø§Ø³ØªØ®Ø¯Ø§Ù… void Ù„ØªÙØ§Ø¯ÙŠ Ù…Ø´Ø§ÙƒÙ„ ESLint Ù…Ø¹ Promise ØºÙŠØ± Ø§Ù„Ù…Ø­Ø¸ÙˆØ¸Ø©
      void processQueue();
    }

  } catch (err) {
    logger.error('âŒ SyncQueue Add Error:', err);
  }
}

/**
 * Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ù†ØªØ¸Ø±Ø© ÙÙŠ Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
 * Ù…Ø¹ Ø­Ù…Ø§ÙŠØ© Ù…Ù† Race Conditions (Ù…Ù†Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©)
 */
async function processQueue() {
  // Ù…Ù†Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø§Ø±ÙŠØ©ØŒ Ù†Ù†ØªØ¸Ø±Ù‡Ø§
  if (isProcessing) {
    if (processingPromise) {
      logger.info('â³ SyncQueue: Already processing, attaching to existing promise.');
      return processingPromise;
    }
    return;
  }

  // ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
  isProcessing = true;

  // Ø¥Ù†Ø´Ø§Ø¡ promise Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­ØªÙ‰ ÙŠÙ…ÙƒÙ† Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
  processingPromise = (async () => {
    try {
      return await _processQueueInternal();
    } finally {
      isProcessing = false;
      processingPromise = null;
    }
  })();

  return processingPromise;
}

/**
 * Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
 */
async function _processQueueInternal() {
  const authStore = useAuthStore();
  const archiveStore = useArchiveStore();
  const syncStore = useSyncStore(); // Use Sync Store
  const { addNotification } = useNotifications();

  // 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆÙˆØ¬ÙˆØ¯ Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø³Ø¬Ù„ Ø¯Ø®ÙˆÙ„
  if (!navigator.onLine || !authStore.isAuthenticated || !authStore.user?.id) {
    syncStore.checkQueue(); // Update status even if we don't process
    return;
  }

  let queue = (await localforage.getItem(QUEUE_KEY)) || [];

  if (queue.length === 0) {
    syncStore.checkQueue();
    return;
  }

  logger.info(`ğŸ”„ Processing sync queue: ${queue.length} item(s)`);

  const syncedArchives = [];
  const deletedArchives = [];
  // const failedItems = []; // No longer needed as separate array

  // We process a COPY allowing us to manipulate the original queue state logically later
  const processingBatch = [...queue];
  const processedIndices = new Set(); // To track what we should remove

  for (let i = 0; i < processingBatch.length; i++) {
    const item = processingBatch[i];

    // --- Backoff Check ---
    // If item failed recently (less than 10 seconds ago), skip it to avoid infinite loop
    const lastAttempt = item.lastAttempt || 0;
    const now = Date.now();
    if (now - lastAttempt < 10000 && item.retryCount > 0) {
      continue; // Skip this item for now
    }

    const type = item.type;
    const source = item.payload || item;
    const date = source.archive_date || source.date;

    try {
      if (type === 'sync_overdue_stores') {
        const payload = item.payload;
        const overdueItems = payload.items || [];
        const archiveDate = payload.archive_date;

        if (!archiveDate) {
          logger.error('âŒ Missing archive_date in overdue sync payload');
          processedIndices.add(i); // Mark for removal (invalid)
          continue;
        }

        const harvestStore = useHarvestStore();
        // Pass a flag to avoid internal queueing on error, we handle it here
        await harvestStore.syncOverdueStoresToCloud(overdueItems, archiveDate, true);

        // Update local metadata
        const localMetadata = await localforage.getItem('overdue_stores_metadata');
        if (localMetadata && localMetadata.archive_date === archiveDate) {
          localMetadata.synced_to_cloud = true;
          await localforage.setItem('overdue_stores_metadata', localMetadata);
        }

        logger.info(`âœ… Synced overdue stores for date: ${archiveDate}`);
        processedIndices.add(i); // Mark for removal (success)

      } else if (type === 'delete_archive') {
        const { error } = await supabase
          .from('daily_archives')
          .delete()
          .eq('user_id', authStore.user.id)
          .eq('archive_date', date);

        if (error) throw error;
        deletedArchives.push(date);
        processedIndices.add(i); // Mark for removal (success)

      } else {
        const { error } = await api.archive.saveDailyArchive(authStore.user.id, date, source.data);
        if (error) throw error;
        syncedArchives.push(date);
        processedIndices.add(i); // Mark for removal (success)
      }
    } catch (err) {
      logger.error(`âŒ Sync failed for [${type}]:`, err);
      // Don't remove from queue. Update retry metadata.
      // We modify the item IN PLACE within the local batch variable, 
      // but we need to ensure this update persists when we merge.
      item.lastAttempt = Date.now();
      item.retryCount = (item.retryCount || 0) + 1;
    }
  }

  // --- CRITICAL: Merge & Save ---
  // We fetch the queue AGAIN to account for items added *while* we were processing
  const currentQueue = (await localforage.getItem(QUEUE_KEY)) || [];

  // Reconstruct the new queue:
  // 1. Keep items from 'currentQueue' that were NOT part of our 'processingBatch' (newly added)
  // 2. Keep items from 'processingBatch' that FAILED (attempted but not in processedIndices)
  //    AND ensure we save their updated metadata (retryCount)
  // 3. Remove items that SUCCEEDED (in processedIndices)

  const newQueue = [];

  // A. Logic to handle items that might have been added/modified concurrently?
  // Since 'addToSyncQueue' appends or updates by (type+date), let's use a unique key map.

  // Helper to generate key
  const getItemKey = (it) => `${it.type}_${(it.payload?.archive_date || it.payload?.date || it.date)}`;

  // Map of our batch status
  const batchMap = new Map();
  processingBatch.forEach((item, idx) => {
    batchMap.set(getItemKey(item), {
      processed: processedIndices.has(idx),
      item: item // This has updated retryCount
    });
  });

  // Iterate over whatever is currently in storage
  for (const storedItem of currentQueue) {
    const key = getItemKey(storedItem);
    const batchInfo = batchMap.get(key);

    if (batchInfo) {
      // This item was part of our batch
      if (!batchInfo.processed) {
        // It failed. Keep it, but use the version with updated retryCount
        newQueue.push(batchInfo.item);
      }
      // If processed, we drop it (it's done)
    } else {
      // This item wasn't in our batch (added recently), keep it
      newQueue.push(storedItem);
    }
  }

  // Also catch edge case where something was in processingBatch but somehow not in currentQueue? 
  // (Unlikely unless 'addToSyncQueue' logic removed it, which it doesn't).

  await localforage.setItem(QUEUE_KEY, newQueue);

  // Update Sync Status UI immediately
  syncStore.checkQueue();

  // 3. Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
  if (syncedArchives.length > 0) {
    addNotification(`ØªÙ… Ù…Ø²Ø§Ù…Ù†Ø© Ø£Ø±Ø´ÙŠÙ: ${syncedArchives.join(', ')} Ø³Ø­Ø§Ø¨ÙŠØ§Ù‹ âœ…`, 'success');
  }

  if (deletedArchives.length > 0) {
    addNotification(`ØªÙ… Ø­Ø°Ù Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®: ${deletedArchives.join(', ')} Ù…Ù† Ø§Ù„Ø³Ø­Ø§Ø¨ ğŸ—‘ï¸`, 'success');
  }

  if (syncedArchives.length > 0 || deletedArchives.length > 0) {
    await archiveStore.loadAvailableDates();
  }

  // Check if we need to schedule another run (if there are failed items waiting for backoff or new items)
  if (newQueue.length > 0) {
    // Check if any item is ready to retry immediately (new items)
    const hasReadyItems = newQueue.some(i => !i.lastAttempt || (Date.now() - i.lastAttempt > 10000));
    if (hasReadyItems) {
      setTimeout(() => { void processQueue(); }, 1000);
    }
  }
}

/**
 * ØªÙ‡ÙŠØ¦Ø© Ù…Ø³ØªÙ…Ø¹ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©
 */
function initializeSyncListener() {
  const syncStore = useSyncStore();

  window.removeEventListener('online', processQueue);
  window.addEventListener('online', processQueue);

  // Initial check
  syncStore.checkQueue();

  // ØªØ´ØºÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ø¨Ø¹Ø¯ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù€ Auth (Ø¨Ø¹Ø¯ 5 Ø«ÙˆØ§Ù†ÙŠ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„)
  setTimeout(processQueue, 5000);
}

/**
 * Ù…Ø³Ø­ Ø§Ù„Ø·Ø§Ø¨ÙˆØ± Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (ÙˆØ¸ÙŠÙØ© Ù„Ù„Ø·ÙˆØ§Ø±Ø¦)
 */
async function clearSyncQueue() {
  const syncStore = useSyncStore();
  try {
    await localforage.removeItem(QUEUE_KEY);
    logger.info('ğŸ—‘ï¸ Sync queue cleared');
    syncStore.checkQueue();
    return true;
  } catch (err) {
    logger.error('âŒ Clear Sync Queue Error:', err);
    return false;
  }
}

export { addToSyncQueue, processQueue, initializeSyncListener, clearSyncQueue };
